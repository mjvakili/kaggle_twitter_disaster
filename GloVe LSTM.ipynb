{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":8,"outputs":[{"output_type":"stream","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\n/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.200d.txt\n/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.50d.txt\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport re  #regular expression\nfrom bs4 import BeautifulSoup\nimport pandas as pd \nfrom sklearn import model_selection, preprocessing\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, Conv1D, MaxPooling1D, Embedding, Dropout, Bidirectional\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras import metrics\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Indexing word vectors.')\n#Many thanks to rtatman for hosting the GloVe word embeddings dataset on Kaggle\n#https://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation\nGLOVE_DIR = '/kaggle/input/glove-global-vectors-for-word-representation/'\nembeddings_index = {}\nwith open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, 'f', sep=' ')\n        embeddings_index[word] = coefs\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":10,"outputs":[{"output_type":"stream","text":"Indexing word vectors.\nFound 400000 word vectors.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Reading the datasets')\ntrain_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","execution_count":11,"outputs":[{"output_type":"stream","text":"Reading the datasets\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Colimns of the training and test datasets are:')\nprint(train_df.keys())\nprint(test_df.keys())","execution_count":12,"outputs":[{"output_type":"stream","text":"Colimns of the training and test datasets are:\nIndex(['id', 'keyword', 'location', 'text', 'target'], dtype='object')\nIndex(['id', 'keyword', 'location', 'text'], dtype='object')\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"url_re = r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))'\n# Thanks to l3nnys for this nice tutorial on text preprocessing\n#https://www.kaggle.com/l3nnys/useful-text-preprocessing-on-the-datasets\n\ndef remove_html(text):\n    '''\n    remove the HTML tags and URLS from the tweets\n    '''\n    if text:\n        # BeautifulSoup on content\n        soup = BeautifulSoup(text, \"html.parser\")\n        # Stripping all <code> tags with their content if any\n        if soup.code:\n            soup.code.decompose()\n        # Get all the text out of the html\n        text =  soup.get_text()\n        # Returning text stripping out all uris\n        return re.sub(url_re, \"\", text)\n    else:\n        return \"\"\n  \ntrain_df['text'] = train_df['text'].map(lambda x: remove_html(x))\ntest_df['text'] = test_df['text'].map(lambda x: remove_html(x))","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing emojis thanks to this instruction on stackoverflow:\n#https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n\ndef remove_emojis(text):\n  emoji_pattern = re.compile(\"[\"\n        r\"\\U0001F600-\\U0001F64F\"  # emoticons\n        r\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        r\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        r\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags=re.UNICODE)\n  text = emoji_pattern.sub(r'', text)# no emoji \n  \n  return text\n\nexample_text = r'This dog is funny \\U0001f602'\ntrain_df['text'] = train_df['text'].map(lambda x: remove_emojis(x))\ntest_df['text'] = test_df['text'].map(lambda x: remove_emojis(x))","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install pyspellchecker","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ndef spell_correct(text):\n  '''\n     check the spellingd with pyspellchecker \n     and replaces the words with incorrect spellings \n     with the most likely correctly spelled candidate\n  '''  \n  text_correct = [spell.correction(x) for x in text.split()]\n  text_correct = \" \".join(str(x) for x in text_correct)\n  \n  return text_correct\n\neasy_example_text = r'What is hapening in the forrestt?'\ndifficult_example_text = r'What is the prive of libery?'\nprint(spell_correct(easy_example_text))\nprint(spell_correct(difficult_example_text))\n\n#Now let's correct the spellings of words in the training and the test set\ntrain_df['text'] = train_df['text'].map(lambda x: spell_correct(x))\ntest_df['text'] = test_df['text'].map(lambda x: spell_correct(x))\n\"\"\"","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"'\\ndef spell_correct(text):\\n  \\'\\'\\'\\n     check the spellingd with pyspellchecker \\n     and replaces the words with incorrect spellings \\n     with the most likely correctly spelled candidate\\n  \\'\\'\\'  \\n  text_correct = [spell.correction(x) for x in text.split()]\\n  text_correct = \" \".join(str(x) for x in text_correct)\\n  \\n  return text_correct\\n\\neasy_example_text = r\\'What is hapening in the forrestt?\\'\\ndifficult_example_text = r\\'What is the prive of libery?\\'\\nprint(spell_correct(easy_example_text))\\nprint(spell_correct(difficult_example_text))\\n\\n#Now let\\'s correct the spellings of words in the training and the test set\\ntrain_df[\\'text\\'] = train_df[\\'text\\'].map(lambda x: spell_correct(x))\\ntest_df[\\'text\\'] = test_df[\\'text\\'].map(lambda x: spell_correct(x))\\n'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = 10000\nembedding_dim = 100\nmax_length = 50\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\nvalidation_split = 0.3\n\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\ntokenizer.fit_on_texts(train_df.text)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\ntraining_sequences = tokenizer.texts_to_sequences(train_df.text)\ntraining_padded = pad_sequences(training_sequences, maxlen = max_length, \n                                padding = padding_type, truncating = trunc_type)\nprint('Shape of the data vector is', training_padded.shape, train_df.target.shape)","execution_count":19,"outputs":[{"output_type":"stream","text":"Found 18099 unique tokens.\nShape of the data vector is (7613, 50) (7613,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Preparing the embedding matrix')\nnum_words = min(vocab_size, len(word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, index in word_index.items():\n  if index >= vocab_size:\n    continue\n  embedding_vector = embeddings_index.get(word)\n  if embedding_vector is not None:\n    embedding_matrix[index] = embedding_vector","execution_count":20,"outputs":[{"output_type":"stream","text":"Preparing the embedding matrix\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_layer = Embedding(num_words, embedding_dim, \n                           embeddings_initializer = Constant(embedding_matrix), \n                           input_length = max_length, \n                           trainable = False)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = model_selection.train_test_split(training_padded, \n                                                                      train_df.target, \n                                                                      test_size = validation_split, \n                                                                      random_state=1)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Copied this from this example on tensorflow website\n#https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\nMETRICS = [\n      metrics.TruePositives(name='tp'),\n      metrics.FalsePositives(name='fp'),\n      metrics.TrueNegatives(name='tn'),\n      metrics.FalseNegatives(name='fn'), \n      metrics.BinaryAccuracy(name='accuracy'),\n      metrics.Precision(name='precision'),\n      metrics.Recall(name='recall'),\n      metrics.AUC(name='auc')]\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_auc', \n    verbose=1,\n    patience=10,\n    mode='max',\n    restore_best_weights=True)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining a Sequential Keras model with Convolution & Global Max Pooling\nsequence_input = Input(shape = (max_length, ))\nembedded_sequences = embedding_layer(sequence_input)\nx = Bidirectional(tf.keras.layers.LSTM(32))(embedded_sequences)\n#x = Conv1D(32, 5, activation='relu')(embedded_sequences)\nx = Dropout(0.5)(x)\nx = Dense(24, activation = 'relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(1, activation = 'sigmoid')(x)\nmodel =  Model(sequence_input, output)\nmodel.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(lr = .0002) ,metrics = METRICS)\nhistory = model.fit(X_train, y_train, batch_size = 64, epochs = 30, \n                    callbacks = [early_stopping],\n                    validation_data = (X_valid, y_valid))","execution_count":null,"outputs":[{"output_type":"stream","text":"Train on 5329 samples, validate on 2284 samples\nEpoch 1/30\n1600/5329 [========>.....................] - ETA: 14s - loss: 0.6904 - tp: 163.0000 - fp: 187.0000 - tn: 722.0000 - fn: 528.0000 - accuracy: 0.5531 - precision: 0.4657 - recall: 0.2359 - auc: 0.5150","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_model_eval(history):\n  '''\n  a simple funtion for model evaluation \n  according to different metrics\n  '''  \n  string = ['loss', 'accuracy', 'precision', 'recall', 'auc', 'tp']  \n  cnt = 0\n  ncols, nrows = 3, 2  \n  fig = plt.figure(constrained_layout=True, figsize = (10,10))\n  gs = gridspec.GridSpec(ncols = 3, nrows = 2, figure = fig)\n  for i in range(nrows):\n    for j in range(ncols):\n      ax = plt.subplot(gs[i,j]) \n      ax.plot(history.history[string[cnt]])\n      ax.plot(history.history['val_'+string[cnt]]) \n      ax.set_xlabel(\"Epochs\")\n      ax.set_ylabel(string[cnt])\n      ax.legend([string[cnt], 'val_'+string[cnt]])\n      cnt +=1\n        \nplot_model_eval(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now let's look at the confusion matrix\npred_valid = model.predict(X_valid)\npred_valid = np.round(pred_valid).astype(int)\nconfusion_matrix(y_valid, pred_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sequences = tokenizer.texts_to_sequences(test_df.text)\ntest_padded = pad_sequences(test_sequences, maxlen = max_length, \n                                padding = padding_type, truncating = trunc_type)\nprint('Shape of the data vector is', test_padded.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(test_padded)\npredictions = np.round(predictions).astype(int).flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsample_submission[\"target\"] = predictions.astype(int)\nprint(sample_submission.head())\nsample_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}