{"cells":[{"metadata":{"_uuid":"b884c807-0c87-4f32-a482-c28ef65ba224","_cell_guid":"afbf9d3e-c20d-4d20-b826-0427933d406f","trusted":true},"cell_type":"code","source":"import numpy as np \nimport re  #regular expression\nfrom bs4 import BeautifulSoup\nimport pandas as pd \nfrom sklearn import model_selection, preprocessing\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, Conv1D, MaxPooling1D, Embedding, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.initializers import Constant","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Indexing word vectors.')\n#Many thanks to rtatman for hosting the GloVe word embeddings dataset on Kaggle\n#https://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation\nGLOVE_DIR = '/kaggle/input/glove-global-vectors-for-word-representation/'\nembeddings_index = {}\nwith open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, 'f', sep=' ')\n        embeddings_index[word] = coefs\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2699c1c5-b89e-4f4d-a60e-0cd63f536d05","_cell_guid":"ac074e5e-2d24-4678-a47a-b4b0e29bf6c7","trusted":true},"cell_type":"code","source":"print('Reading the datasets')\ntrain_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a69a1c16-9a48-4fa0-a1ac-be83eb087729","_cell_guid":"99b32dbf-d105-4150-a63e-fc59d35d0a4a","trusted":true},"cell_type":"code","source":"print('Colimns of the training and test datasets are:')\nprint(train_df.keys())\nprint(test_df.keys())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"921e7ea6-d1d4-4079-8f1b-0356b106abe3","_cell_guid":"a2302a51-90ae-4d46-9c82-24d168d8d4eb","trusted":true},"cell_type":"code","source":"url_re = r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))'\n# Thanks to l3nnys for this nice tutorial on text preprocessing\n#https://www.kaggle.com/l3nnys/useful-text-preprocessing-on-the-datasets\n\ndef remove_html(text):\n    '''\n    remove the HTML tags and URLS from the tweets\n    '''\n    if text:\n        # BeautifulSoup on content\n        soup = BeautifulSoup(text, \"html.parser\")\n        # Stripping all <code> tags with their content if any\n        if soup.code:\n            soup.code.decompose()\n        # Get all the text out of the html\n        text =  soup.get_text()\n        # Returning text stripping out all uris\n        return re.sub(url_re, \"\", text)\n    else:\n        return \"\"\n  \ntrain_df['text'] = train_df['text'].map(lambda x: remove_html(x))\ntest_df['text'] = test_df['text'].map(lambda x: remove_html(x))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ba32e1e-d60a-4af4-aee5-f5d765993715","_cell_guid":"61f8dd81-cc06-4a5a-84b3-d6ae865d83bf","trusted":true},"cell_type":"code","source":"# Removing emojis thanks to this instruction on stackoverflow:\n#https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n\ndef remove_emojis(text):\n  emoji_pattern = re.compile(\"[\"\n        r\"\\U0001F600-\\U0001F64F\"  # emoticons\n        r\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        r\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        r\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags=re.UNICODE)\n  text = emoji_pattern.sub(r'', text)# no emoji \n  \n  return text\n\nexample_text = r'This dog is funny \\U0001f602'\ntrain_df['text'] = train_df['text'].map(lambda x: remove_emojis(x))\ntest_df['text'] = test_df['text'].map(lambda x: remove_emojis(x))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ffbf005-7843-4dde-99ab-9b3b95f6fd86","_cell_guid":"caee2166-b44d-4cd3-afc4-f656a1ced751","trusted":true},"cell_type":"code","source":"vocab_size = 20000\nembedding_dim = 100\nmax_length = 1000\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\nvalidation_split = 0.2\n\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\ntokenizer.fit_on_texts(train_df.text)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\ntraining_sequences = tokenizer.texts_to_sequences(train_df.text)\ntraining_padded = pad_sequences(training_sequences, maxlen = max_length, \n                                padding = padding_type, truncating = trunc_type)\nprint('Shape of the data vector is', training_padded.shape, train_df.target.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22e061f0-26cc-4866-a608-ba6e7c77a404","_cell_guid":"4328f0b9-60a5-408e-9ccb-7f5576a40543","trusted":true},"cell_type":"code","source":"print('Preparing the embedding matrix')\nnum_words = min(vocab_size, len(word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, index in word_index.items():\n  if index >= vocab_size:\n    continue\n  embedding_vector = embeddings_index.get(word)\n  if embedding_vector is not None:\n    embedding_matrix[index] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ef1b482-18e8-4332-87dd-8577287979fd","_cell_guid":"2c7a1ee7-6eaa-40ab-86ed-ecf3830880b5","trusted":true},"cell_type":"code","source":"embedding_layer = Embedding(num_words, embedding_dim, \n                           embeddings_initializer = Constant(embedding_matrix), \n                           input_length = max_length, \n                           trainable = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"affd09b3-f374-47e5-8e69-546f6fce558f","_cell_guid":"100def39-5aed-4470-8e52-7719dcc0b900","trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = model_selection.train_test_split(training_padded, \n                                                                      train_df.target, \n                                                                      test_size = validation_split, \n                                                                      random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e6081c7-db29-4fdf-b20d-01f31884b40f","_cell_guid":"a9a5b5c3-fe80-4a08-8fa6-538f7fe9594d","trusted":true},"cell_type":"code","source":"#Defining a Sequential Keras model with Convolution & Global Max Pooling\nsequence_input = Input(shape = (max_length, ))\nembedded_sequences = embedding_layer(sequence_input)\nx = Conv1D(32, 5, activation='relu')(embedded_sequences)\nx = GlobalMaxPooling1D()(x)\nx = Dropout(0.5)(x)\nx = Dense(24, activation = 'relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(1, activation = 'sigmoid')(x)\nmodel =  Model(sequence_input, output)\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, batch_size = 32, epochs = 10, validation_data = (X_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cf3b6e7-8280-4572-a61a-d61d6be112ad","_cell_guid":"bb83e318-010b-4b2b-a94a-4706005c815c","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n\nplot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sequences = tokenizer.texts_to_sequences(test_df.text)\ntest_padded = pad_sequences(test_sequences, maxlen = max_length, \n                                padding = padding_type, truncating = trunc_type)\nprint('Shape of the data vector is', test_padded.shape, test_df.target.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c4d0c52-7de7-4dfb-a0b3-b8c6b83cfcdf","_cell_guid":"075e8747-1275-47cc-8ab9-590efb41c1cd","trusted":true},"cell_type":"code","source":"#sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e38c9f87-9dc7-4d52-9db0-f47b17ae0406","_cell_guid":"16b80e6b-0550-4abc-bb81-246e3490ef8e","trusted":true},"cell_type":"code","source":"#sample_submission[\"target\"] = model.predict(test_padded)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b6eb314-e6aa-453e-a90e-acb38822019e","_cell_guid":"b92a113b-6c33-4568-8645-a816636dbe38","trusted":true},"cell_type":"code","source":"#sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"027314d7-ec2b-4aed-b1db-d6f4466fc5fd","_cell_guid":"7eb12df9-8fc4-4759-ae2d-71f2edf79760","trusted":true},"cell_type":"code","source":"#sample_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}